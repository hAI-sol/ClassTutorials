{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision \n",
    "\n",
    "# Exercise 7: Feature Matching\n",
    "\n",
    "- TU Chemnitz\n",
    "    - Fak. für Informatik\n",
    "        - Professur Künstliche Intelligenz\n",
    "            - Lehre\n",
    "                - Bildverstehen\n",
    "     \n",
    "Contact:\n",
    "* julien dot vitay at informatik dot tu-chemnitz dot de\n",
    "* abbas dot al-ali at informatik dot tu-chemnitz dot de\n",
    "\n",
    "Course web page:\n",
    "[https://www.tu-chemnitz.de/informatik/KI/edu/biver/](https://www.tu-chemnitz.de/informatik/KI/edu/biver/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harris Corner detection\n",
    "\n",
    "<img src=\"img/castle-harris.png\" alt=\"img/castle-harris.png\" width=\"400\"/>   \n",
    "\n",
    "1. Compute the gradient at each location:\n",
    "\n",
    "$$\n",
    "    \\mathbf{\\nabla} I(x, y) = \\begin{bmatrix} \\frac{\\partial I(x , y)}{\\partial x} &\\frac{\\partial I(x , y)}{\\partial y} \\\\ \\end{bmatrix} = \\begin{bmatrix} I_x & I_y \\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Compute the auto-correlation matrix at each location by using a window function: \n",
    "\n",
    "$$\n",
    "    \\mathbf{A} = \\sum_{x, y} w(x, y)  \\begin{bmatrix} I_x^2 & I_x I_y \\\\ I_x I_y & I_y^2\\\\ \\end{bmatrix} = w \\ast \\begin{bmatrix} I_x^2 & I_x I_y \\\\ I_x I_y & I_y^2\\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Determine the Harris score at each location:\n",
    "\n",
    "$$\n",
    "    f(x,y) = \\text{det}(\\mathbf{A}) - \\kappa \\,  \\text{tr}(\\mathbf{A})^2\n",
    "$$\n",
    "\n",
    "4. Threshold the Harris score to keep only the most interesting points.\n",
    "\n",
    "5. Suppress the local non-maxima to keep a single point in each blob.\n",
    "\n",
    "\n",
    "* The three first steps of Harris corner detection are provided by the `cv2.cornerHarris()` method:\n",
    "\n",
    "```python\n",
    "score = cv2.cornerHarris(img, blockSize=2, ksize=3, k=0.04)\n",
    "```\n",
    "\n",
    "* `img` must be a grayscale 32-bits floating-point representation of the image (`gray.astype(np.float32)`).\n",
    "\n",
    "* `blockSize` defines the size of the window function.\n",
    "\n",
    "* `ksize` is the size of the Sobel filters used to compute the gradients.\n",
    "\n",
    "* `k` is the $\\kappa$ parameter of the Harris score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1:** Harris corner detection\n",
    "\n",
    "1. Visualize the Harris score of the images `star.jpg` and `castle.jpg`. Display the range of values by using `plt.colorbar()`.\n",
    "\n",
    "2. Vary the `blockSize` and `ksize` parameters. Conclude on their effect on corner detection.\n",
    "\n",
    "3. The Harris score has to be thresholded. A rule of thumb is to keep only scores higher than 1% of the maximum score (`score.max()`). Use `cv2.threshold` on the Harris score and visualize the result. \n",
    "\n",
    "4. Vary the threshold (0.1%, 10%...) and conclude on its importance for corner detection. \n",
    "\n",
    "- Several methods are available for non-maxima suppression. Here we will compute the center of each **blob** of points in the thresholded score image: \n",
    "\n",
    "```python\n",
    "_, _, _, corners = cv2.connectedComponentsWithStats(thresh_score.astype(np.uint8))\n",
    "```\n",
    "\n",
    "5. Print the number of the corners (use the `shape` of `corners`). Visualize the corners. Add them the original image to see to what they correspond.\n",
    "\n",
    "6. Now that you have the complete Harris detector, vary again the various parameters (`blockSize`, `ksize`, `k` and the threshold percentage) to see how they influence corner detection. Search for corners in other images.\n",
    "\n",
    "\n",
    "**Note:** it would be a better practice to refine the position of the corners using `cv2.cornerSubPix`:\n",
    "\n",
    "```python\n",
    "_, _, _, centroids = cv2.connectedComponentsWithStats(thresh_score.astype(np.uint8))\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv2.cornerSubPix(gray, centroids.astype(np.float32), (5,5), (-1, -1), criteria)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Harris\n",
    "\n",
    "# Load the image \n",
    "img = cv2.imread('castle.jpg')\n",
    "#img = cv2.imread('star.jpg')\n",
    "\n",
    "# Convert to float32 gray\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "\n",
    "# Compute the Harris score\n",
    "score = cv2.cornerHarris(gray, blockSize=2, ksize=3, k=0.04)\n",
    "\n",
    "# Show the score\n",
    "plt.figure()\n",
    "plt.imshow(score, cmap=plt.cm.gray)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.colorbar()\n",
    "plt.title('Harris Score')\n",
    "plt.show()\n",
    "\n",
    "# Threshold the score\n",
    "ret, thresh_score = cv2.threshold(score, 0.01*score.max(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(thresh_score, cmap=plt.cm.gray)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.title('Thresholded Harris Score')\n",
    "plt.show()\n",
    "\n",
    "# Non-maxima suppression using centroids\n",
    "_, _, _, corners = cv2.connectedComponentsWithStats(thresh_score.astype(np.uint8))\n",
    "print(corners.shape[0], 'corners detected.')\n",
    "\n",
    "\"\"\"\n",
    "_, _, _, centroids = cv2.connectedComponentsWithStats(thresh_score.astype(np.uint8))\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv2.cornerSubPix(gray, centroids.astype(np.float32), (5,5), (-1, -1), criteria)\n",
    "print(corners.shape[0], 'corners detected.')\n",
    "\"\"\"\n",
    "\n",
    "plt.figure()\n",
    "for c in range(corners.shape[0]):\n",
    "    plt.plot(corners[c, 0], corners[c, 1], '.b')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.xlim((0, img.shape[1]-1))\n",
    "plt.ylim((img.shape[0]-1, 0))\n",
    "plt.title('Non-maxima suppression')\n",
    "plt.show()\n",
    "\n",
    "# Plot the result\n",
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "for c in range(corners.shape[0]):\n",
    "    plt.plot(corners[c, 0], corners[c, 1], '.b')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.title('Harris detection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template matching\n",
    "\n",
    "<img src=\"img/matching.png\" alt=\"img/matching.png\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of feature matching\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/featurematching.png\" alt=\"img/featurematching.png\" width=\"400\"/> \n",
    "  <figcaption>Source: J. Hays</figcaption>\n",
    "</figure> \n",
    "\n",
    "**1 - Feature detection**\n",
    "\n",
    "1. Find a set of interest points or **features**.\n",
    "\n",
    "2. Define a region around each point.\n",
    "\n",
    "$\\rightarrow$ *Harris Corner detection, LoG detection, Suboctave DoG pyramid...*\n",
    "\n",
    "**2 - Feature description**\n",
    "\n",
    "3. Extract and normalize the region content.\n",
    "\n",
    "4. Compute a local descriptor.\n",
    "\n",
    "$\\rightarrow$ *SIFT, SURF, GLOH, ORB, HOG, PCA...*\n",
    "\n",
    "**3 - Feature matching**\n",
    "\n",
    "5. Match local descriptors between images. \n",
    "\n",
    "$\\rightarrow$ *Nearest neighbors, K-d trees...*\n",
    "\n",
    "**4 - Feature alignment**\n",
    "\n",
    "6. Estimate the transformation between the features. \n",
    "\n",
    "$\\rightarrow$ *Least squares, RANSAC...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of template matching\n",
    "\n",
    "**1 - Feature detection**\n",
    "\n",
    "* Detect interest points in the *template* and *scene* images using a **Suboctave DoG pyramid**.\n",
    "\n",
    "**2 - Feature description**\n",
    "\n",
    "* Compute the **ORB** representation of the features.\n",
    "\n",
    "**3 - Feature matching**\n",
    "\n",
    "* Match the features of the template and scene using **Nearest neighbor distance ratio matching**.\n",
    "\n",
    "**4 - Feature alignment**\n",
    "\n",
    "* Estimate the transformation and reject outliers with **RANSAC**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB feature detection and description\n",
    "\n",
    "* **ORB** (Oriented FAST and Rotated BRIEF) is a fast and free alternative to SIFT and SURF feature descriptors.\n",
    "\n",
    "<http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html>\n",
    "\n",
    "* Interest points are detected using the FAST algorithm:\n",
    "\n",
    "<http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_fast/py_fast.html>\n",
    "\n",
    "* Feature descriptions are computed using the BRIEF (Binary Robust Independent Elementary Features) method:\n",
    "\n",
    "<http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_brief/py_brief.html>\n",
    "\n",
    "* In the end, the only differences with SIFT are:\n",
    "\n",
    "    1. A much faster algorithm.\n",
    "\n",
    "    2. 32-bits descriptors instead of 128-bits.\n",
    "\n",
    "    3. A patent-free method that works almost as well as SIFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2:**  Feature detection and ORB description\n",
    "\n",
    "```python\n",
    "template = cv2.imread('beer.png',0)\n",
    "scene = cv2.imread('scene.jpg',0)\n",
    "```\n",
    "\n",
    "* The OpenCV ``cv2.ORB_create()`` method creates a ORB object that both **detects** interest points using Suboctave DoG pyramids and **computes** a description vector.\n",
    "\n",
    "\n",
    "```python\n",
    "orb = cv2.ORB_create(nfeatures=1500, scaleFactor=1.3, WTA_K=2, nlevels=8, patchSize=31)\n",
    "keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "```\n",
    "\n",
    "* To see what the parameters mean, check the doc: <http://docs.opencv.org/trunk/db/d95/classcv_1_1ORB.html>\n",
    "\n",
    "\n",
    "1. For the template and scene images, visualize what `keypoints` and `descriptors` represent (dimensions...). \n",
    "\n",
    "2. To visualize the keypoints of each image, use the `cv2.drawKeypoints()` method and show the resulting image:\n",
    "\n",
    "```python\n",
    "img_with_keypoints = cv2.drawKeypoints(img, keypoints, None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images\n",
    "template = cv2.imread('beer.png',0)\n",
    "scene = cv2.imread('scene.jpg',0)\n",
    "\n",
    "# Initialize a ORB detector\n",
    "orb = cv2.ORB_create(nfeatures=1500, scaleFactor=1.3, WTA_K=2, nlevels=8, patchSize=31)\n",
    "\n",
    "# Find the keypoints and descriptors with ORB\n",
    "template_keypoints, template_descriptors = orb.detectAndCompute(template, None)\n",
    "scene_keypoints, scene_descriptors = orb.detectAndCompute(scene, None)\n",
    "\n",
    "# Draw the keypoints on the images\n",
    "template_with_keypoints = cv2.drawKeypoints(template, template_keypoints, None)\n",
    "scene_with_keypoints = cv2.drawKeypoints(scene, scene_keypoints, None)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(template_with_keypoints)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('template with keypoints')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(scene_with_keypoints)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('scene with keypoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3:** Feature matching\n",
    "\n",
    "1. OpenCV provides a **brute-force matcher** based on **k-nearest neighbors** and a **FLANN** matcher based on **k-d trees**. We will use the brute force matcher here: \n",
    "\n",
    "```python\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "matches = bf.knnMatch(template_descriptors, scene_descriptors, k=2)\n",
    "```\n",
    "\n",
    "* Here we get a list of the **2** nearest neighbours for each feature.\n",
    "\n",
    "* We use the **Hamming distance**, as we have ORB feature vectors. SIFT features would only require the Euclidian distance.\n",
    "\n",
    "\n",
    "2. To visualize the matches between the template and the scene, use the `cv2.drawMatchesKnn` method:\n",
    "\n",
    "```python\n",
    "image_with_matches = cv2.drawMatchesKnn(\n",
    "    template, # First image\n",
    "    template_keypoints, # Keypoints of the first image\n",
    "    scene, # Second image\n",
    "    scene_keypoints, # Keypoints of the second image\n",
    "    matches, # Detected matches\n",
    "    None, # Leave it\n",
    "    flags=2 # What to plot\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute Force Matcher with default params\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "matches = bf.knnMatch(template_descriptors, scene_descriptors, k=2)\n",
    "\n",
    "image_with_matches = cv2.drawMatchesKnn(\n",
    "    template, template_keypoints,\n",
    "    scene, scene_keypoints, matches, None, flags=2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_with_matches)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 4:** Keeping only \"good\" matches\n",
    "\n",
    "\n",
    "* A match is good if and only if it is the only possible, i.e. the second nearest neighbor is far away from the first nearest.\n",
    "\n",
    "1. Reject matches that do not fit the **Nearest neighbor distance ratio matching** criteria:\n",
    "\n",
    "```python\n",
    "good_matches = []\n",
    "for first, second in matches:\n",
    "    if first.distance < 0.9 * second.distance:\n",
    "        good_matches.append([first])\n",
    "```\n",
    "\n",
    "2. How many good matches do you have? Print their number, and a message if you have less than 10 good matches.\n",
    "\n",
    "3. Visualize the matches. Are the matches good enough? Are there outliers? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ratio test to keep only good neighbours\n",
    "good_matches = []\n",
    "for first, second in matches:\n",
    "    if first.distance < 0.9 * second.distance:\n",
    "        good_matches.append([first])\n",
    "print(\"Found\", len(good_matches), 'matches.')\n",
    "\n",
    "# a meesage when not enough matches\n",
    "if len(good_matches) < 10:\n",
    "    print('Less than 10 good matches')\n",
    "\n",
    "image_with_good_matches = cv2.drawMatchesKnn(\n",
    "    template, template_keypoints, scene, scene_keypoints,\n",
    "    good_matches, None, flags=2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_with_good_matches)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5:** Feature alignment\n",
    "\n",
    "1. The next step is to use RANSAC to estimate the projective transformation (*homography*) between the template and the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the keypoints\n",
    "template_points = np.float32(\n",
    "    [ template_keypoints[m[0].queryIdx].pt for m in good_matches ]\n",
    "                            ).reshape(-1,1,2)\n",
    "scene_points = np.float32(\n",
    "    [ scene_keypoints[m[0].trainIdx].pt for m in good_matches ]\n",
    "                         ).reshape(-1,1,2)\n",
    "\n",
    "# Find the homography with RANSAC\n",
    "M, mask = cv2.findHomography(template_points, scene_points, cv2.RANSAC, ransacReprojThreshold=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyse the estimated projection between the two sets of points.\n",
    "\n",
    "3. `mask` returns for all matches of `good_matches` 1 if the match is an inlier, 0 if it is an outlier. Build a new list `inliers` as a sub-list of `good_matches` where only the inliers are kept.\n",
    "\n",
    "4. Visualize the inlier matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('M:\\n',M)\n",
    "\n",
    "# Find the inliers\n",
    "inliers = []\n",
    "for idx, match in enumerate(good_matches):\n",
    "    if mask[idx, 0] == 1:\n",
    "        inliers.append(match)\n",
    "        \n",
    "# Draw the inlier matches\n",
    "image_with_inliers = cv2.drawMatchesKnn(\n",
    "    template, template_keypoints, scene, scene_keypoints, inliers, None, flags=2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_with_inliers)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5:** Transforming borders of the template\n",
    "\n",
    "1. Find the borders of the template:\n",
    "\n",
    "```python\n",
    "height, width = template.shape\n",
    "rectangle = np.float32([ [0, 0], [0, height-1], \n",
    "                         [width-1, height-1], [width-1, 0] ]).reshape(-1,1,2)\n",
    "```\n",
    "\n",
    "2. Apply the projective transformation on it:\n",
    "\n",
    "```python\n",
    "warped = cv2.perspectiveTransform(rectangle, M)\n",
    "```\n",
    "\n",
    "3. Add the warped rectangle to a copy of the scene and visualize the result:\n",
    "\n",
    "```python\n",
    "scene2 = scene.copy()\n",
    "cv2.polylines(scene2, [np.int32(warped)], True, 255, 3, cv2.LINE_AA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the perspective transformation on the borders of the template\n",
    "height, width = template.shape\n",
    "rectangle = np.float32(\n",
    "    [ [0, 0], [0, height-1], [width-1, height-1], [width-1, 0] ]).reshape(-1,1,2)\n",
    "\n",
    "warped = cv2.perspectiveTransform(rectangle, M)\n",
    "scene2 = scene.copy()\n",
    "\n",
    "cv2.polylines(scene2, [np.int32(warped)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "# Draw the inlier matches\n",
    "image_with_inliers = cv2.drawMatchesKnn(\n",
    "    template, template_keypoints, scene2, scene_keypoints, inliers, None, flags=2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_with_inliers)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 6 (Optional):** Try other images\n",
    "\n",
    "1. Search for other template/scene images and search the templates in the scene. Is it a robust method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
