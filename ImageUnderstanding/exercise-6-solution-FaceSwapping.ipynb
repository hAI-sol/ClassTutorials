{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision \n",
    "\n",
    "# Exercise 6: Face Swapping\n",
    "\n",
    "- TU Chemnitz\n",
    "    - Fak. für Informatik\n",
    "        - Professur Künstliche Intelligenz\n",
    "            - Lehre\n",
    "                - Bildverstehen\n",
    "     \n",
    "Contact:\n",
    "* julien dot vitay at informatik dot tu-chemnitz dot de\n",
    "* abbas dot al-ali at informatik dot tu-chemnitz dot de\n",
    "\n",
    "Course web page:\n",
    "[https://www.tu-chemnitz.de/informatik/KI/edu/biver/](https://www.tu-chemnitz.de/informatik/KI/edu/biver/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Swapping\n",
    "\n",
    "<img src=\"img/switchingeds.jpg\" alt=\"img/switchingeds.jpg\" width=\"600\"/>\n",
    "\n",
    "* The code for the exercise is adapted from the blog post of Matthew Earl:\n",
    "\n",
    "<http://matthewearl.github.io/2015/07/28/switching-eds-with-python> \n",
    "\n",
    "* Matthew Earl even provides the complete Python code for it:\n",
    "\n",
    "<https://github.com/matthewearl/faceswap/blob/master/faceswap.py>\n",
    "\n",
    "* See the copyright notice at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The goal of this exercise is to reimplement it step by step to better understand it. Don't hesitate to read the blogpost for additional information.\n",
    "\n",
    "<table> \n",
    "  <tr>\n",
    "    <td>\n",
    "        <img src=\"img/swap-output.jpg\" alt=\"img/swap-output.jpg\" width=\"250\"/>        \n",
    "    </td>      \n",
    "    <td>\n",
    "        <img src=\"img/merkel_clinton.jpg\" alt=\"img/merkel_clinton.jpg\" width=\"200\"/>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Swapping algorithm\n",
    "\n",
    "1. Detect landmarks in both images.\n",
    "\n",
    "2. Compute a binary mask over the two faces (eyes + nose + mouth).\n",
    "\n",
    "3. Estimate the affine transformation between the two masks using **Procrustes analysis**.\n",
    "\n",
    "4. Inverse warping of the source face to align the facial features with the target head.\n",
    "\n",
    "5. Crop the warped image to the target mask. \n",
    "\n",
    "6. Add the cropped image to the target image.\n",
    "\n",
    "7. Adapt the colors using **Gaussian color correction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1:** Installing `dlib`\n",
    "\n",
    "1. Download dlib at <http://dlib.net>:\n",
    "\n",
    "    - dlib-19.2 : <http://dlib.net/files/dlib-19.2.tar.bz2>\n",
    "    - dlib-19.16: <http://dlib.net/files/dlib-19.16.tar.bz2>\n",
    "\n",
    "2. Unzip it:\n",
    "\n",
    "```\n",
    "tar xvjf dlib-19.2.tar.bz2\n",
    "tar xvjf dlib-19.16.tar.bz2\n",
    "```\n",
    "\n",
    "3. Compile and install it (takes a couple of minutes):\n",
    "\n",
    "```\n",
    "cd dlib-19.2\n",
    "cd dlib-19.16\n",
    "python setup.py install --user\n",
    "```\n",
    "\n",
    "4. You will also need the data for the landmark detector:\n",
    "\n",
    "<http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2>\n",
    "\n",
    "5. Unzip it and place the `.dat` file in the same directory as your notebook\n",
    "\n",
    "```\n",
    "unzip shape_predictor_68_face_landmarks.dat.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2:** Opening images\n",
    "\n",
    "\n",
    "1. Read the two images:\n",
    "    - `source` <== `clinton.jpg`: contains the face\n",
    "    - `destination` <== `merkel.jpg`: contains the head \n",
    "    \n",
    "\n",
    "2. Convert them to RGB so that `matplotlib` can display them well (do not use `opencv` for displaying)\n",
    "\n",
    "3. Display the two images side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.imread('clinton.jpg')\n",
    "source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "destination = cv2.imread('merkel.jpg')\n",
    "destination = cv2.cvtColor(destination, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(source)\n",
    "plt.xticks([]), plt.yticks([]) \n",
    "plt.title('Source')\n",
    "plt.subplot(122)\n",
    "plt.imshow(destination)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.title('Destination')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3:** Face detection\n",
    "\n",
    "<img src=\"img/figure_2.png\" alt=\"img/figure_2.png\" width=\"600\"/> \n",
    "\n",
    "* The second step is to detect where the face is in each image (if any).\n",
    "\n",
    "* `dlib` provides a fast face detector based on the **Viola-Jones** face detection algorithm\n",
    "(<https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework>).\n",
    "\n",
    "* The detector was trained on a huge number of frontal faces.\n",
    "\n",
    "* OpenCV also has one (`cvHaarDetectObjects()`), but let's use dlib. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a `detector` object:\n",
    "\n",
    "```python\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "```\n",
    "\n",
    "2. Apply the face detector on the `source` image:\n",
    "\n",
    "```python\n",
    "source_facerect = detector(source)[0]\n",
    "```\n",
    "\n",
    "- It returns a list a bounding boxes, one per face detected in the image (hence the `[0]` as we have only one face).\n",
    "\n",
    "- Each bounding box is a `dlib.rectangle` object, which allows to get the coordinates (`top()`, `bottom()`, `left()`, `right()`) and the dimensions (`width()`, `height()`) of the bounding box in the image.\n",
    "\n",
    "3. You can print the bounding box to see the coordinates of the top-left and bottom-right points:\n",
    "\n",
    "```python\n",
    "print(source_facerect)\n",
    "```\n",
    "\n",
    "4. Visualize the box, you need to use the axes `ax` returned by `subplot()`:\n",
    "\n",
    "```python\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(source)\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (source_facerect.left(), source_facerect.top()), # Top-left corner\n",
    "        source_facerect.width(), # Width\n",
    "        source_facerect.height(), # Height\n",
    "        edgecolor='r', lw=1.0, fill=False))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "```\n",
    "\n",
    "5. Repeat the same for the `destination`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "source_facerect = detector(source)[0]\n",
    "destination_facerect = detector(destination)[0]\n",
    "\n",
    "print(source_facerect)\n",
    "print(destination_facerect)\n",
    "# [(321, 321) (692, 692)]\n",
    "# [(362, 280) (734, 651)]\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(source)\n",
    "ax.add_patch(plt.Rectangle(\n",
    "    (source_facerect.left(), source_facerect.top()), # Top-left corner\n",
    "    source_facerect.width(), # Width\n",
    "    source_facerect.height(), # Height\n",
    "    edgecolor='r', lw=1.0, fill=False))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(destination)\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (destination_facerect.left(), destination_facerect.top()), # Top-left corner\n",
    "        destination_facerect.width(), # Width\n",
    "        destination_facerect.height(), # Height\n",
    "        edgecolor='r', lw=1.0, fill=False))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 4 (Optional):** Using the face detector on multiple faces\n",
    "\n",
    "<img src=\"img/crowd.png\" alt=\"img/crowd.png\" width=\"600\"/> \n",
    "\n",
    "- Apply the face detector on the image `crowd.jpg` and visualize all the bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd = cv2.imread('crowd.jpg')\n",
    "crowd = cv2.cvtColor(crowd, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "crowd_facerect = detector(crowd)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "plt.imshow(crowd)\n",
    "\n",
    "for rect in crowd_facerect:\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle(\n",
    "        (rect.left(), rect.top()), # Top-left\n",
    "        rect.width(), # Width\n",
    "        rect.height(), # Height\n",
    "        edgecolor='r', lw=1.0, fill=False))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5:** Landmark extraction\n",
    "\n",
    "<img src=\"img/landmarks.png\" alt=\"img/landmarks.png\" width=\"300\"/> \n",
    "\n",
    "* Once the face region is extracted, one can pass it to a **feature extractor** that will *annotate* the relevant landmarks in the face.\n",
    "\n",
    "* The feature extractor proposed by dlib uses the complex algorithm described in *One Millisecond Face Alignment with an Ensemble of Regression Trees*, by Vahid Kazemi and Josephine Sullivan.\n",
    "\n",
    "* Its usage is very simple:\n",
    "\n",
    "```python\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "source_landmarks = []\n",
    "for p in predictor(source, source_facerect).parts():\n",
    "    source_landmarks.append([p.x, p.y])\n",
    "source_landmarks = np.array(source_landmarks, dtype=np.int)\n",
    "```\n",
    "\n",
    "`source_landmarks` is then a Numpy array with the $(x, y)$ coordinates of 68 different landmarks.\n",
    "\n",
    "* The file `shape_predictor_68_face_landmarks.dat` must be in the same directory as your script, otherwise give a relative path.\n",
    "\n",
    "* Each landmark represents a specific part of a face (jaw, eyebrows, eyes, nose, mouth).\n",
    "\n",
    "\n",
    "1. Visualize the landmarks of the two images. You just need to plot the two columns of the landmarks array against each other.\n",
    "\n",
    "2. Optional: you can also visualize their numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "source_landmarks = []\n",
    "for p in predictor(source, source_facerect).parts():\n",
    "    source_landmarks.append([p.x, p.y])\n",
    "source_landmarks = np.array(source_landmarks, dtype=np.int)\n",
    "\n",
    "destination_landmarks = []\n",
    "for p in predictor(destination, destination_facerect).parts():\n",
    "    destination_landmarks.append([p.x, p.y])\n",
    "destination_landmarks = np.array(destination_landmarks, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altenatively load them\n",
    "# source_landmarks = np.load('source_landmarks.npy')\n",
    "# destination_landmarks = np.load('destination_landmarks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only the face landmarks\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(source)\n",
    "plt.plot(source_landmarks[:, 0], source_landmarks[:, 1], '.')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(destination)\n",
    "plt.plot(destination_landmarks[:, 0], destination_landmarks[:, 1], '.')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The landmarks can be grouped into regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_POINTS = list(range(17, 68))\n",
    "MOUTH_POINTS = list(range(48, 61))\n",
    "RIGHT_BROW_POINTS = list(range(17, 22))\n",
    "LEFT_BROW_POINTS = list(range(22, 27))\n",
    "RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "LEFT_EYE_POINTS = list(range(42, 48))\n",
    "NOSE_POINTS = list(range(27, 35))\n",
    "JAW_POINTS = list(range(0, 17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The next step is to align the two images by estimating the similarity transformation between the two sets of points.\n",
    "\n",
    " - we select only the inner points (not the jaw):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points used to line up the images\n",
    "ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS +\n",
    "                               RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot only the face landmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only the face landmarks used to line up the images\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(255*np.ones(source.shape))\n",
    "plt.plot(source_landmarks[ALIGN_POINTS, 0], source_landmarks[ALIGN_POINTS, 1], '.')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(255*np.ones(destination.shape))\n",
    "plt.plot(destination_landmarks[ALIGN_POINTS, 0], destination_landmarks[ALIGN_POINTS, 1], '.')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 6:**  Affine transformation estimation\n",
    "\n",
    "<!--(img/figure_4.png)-->\n",
    "\n",
    "* Now that we have the coordinates of interest points in the two images, we need to estimate the similarity transformation betwen the two.\n",
    "\n",
    "* Possible methods include: *least squares method*, *RANSAC* and *Procrustes analysis*. \n",
    "\n",
    "* Here Matthew Earl chose to use **Procrustes analysis**, so we do it too ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procrustes Analysis\n",
    "\n",
    "- The idea behind **Procrustes analysis** is to estimate separately the *translation*, *rotation* and *scaling* components of the similarity transformation.\n",
    "\n",
    "- We search for a translation vector $\\mathbf{t}$, a rotation matrix $\\mathbf{R}$ and a scaling factor $s$ so that the **least squares** function is minimized:\n",
    "\n",
    "$$\n",
    "    E_\\text{LS}(\\mathbf{t}, \\mathbf{R}, s) = \\sum_{i=1}^{68} || s \\, \\mathbf{R} \\, \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q_i}||^2\n",
    "$$\n",
    "\n",
    "with $\\mathbf{p}_i$ and $\\mathbf{q}_i$ being the coordinates of the landmarks in the source and destination images respectively.\n",
    "\n",
    "- The details of Procrustes analysis are irrelevant for this exercise. The function:\n",
    "```python\n",
    "transformation_from_points(points1, points2)\n",
    "```\n",
    "provided here is designed to do the job, let's just notice that it depends on basic geometrical analysis and *singular value decomposition*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_from_points(points1, points2):\n",
    "    points1 = np.matrix(points1).astype(np.float64)\n",
    "    points2 = np.matrix(points2).astype(np.float64)\n",
    "    # The translation t corresponds to the displacement of the centers of mass\n",
    "    c1 = np.mean(points1, axis=0)\n",
    "    c2 = np.mean(points2, axis=0)\n",
    "    # Normalize the mean of the points\n",
    "    points1 -= c1\n",
    "    points2 -= c2\n",
    "\n",
    "    # The scaling corresponds to the ratio between the standard deviations\n",
    "    s1 = np.std(points1)\n",
    "    s2 = np.std(points2)\n",
    "    # Normalize the variance of the points\n",
    "    points1 /= s1\n",
    "    points2 /= s2\n",
    "    # Apply Singular Value decomposition on the correlation matrix of the points\n",
    "    U, S, Vt = np.linalg.svd(points2.T * points1)\n",
    "    # The R we seek is in fact the transpose of the one given by U * Vt.\n",
    "    R = (U * Vt).T\n",
    "    # Return the affine transformation matrix\n",
    "    return np.hstack(((s1 / s2) * R, c1.T - (s1 / s2) * R * c2.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply Procrustes analysis to find the similarity transformation between the inner landmarks of the source image and the inner landmarks of the destination image:\n",
    "\n",
    "```python\n",
    "M = transformation_from_points(\n",
    "        source_landmarks[ALIGN_POINTS],\n",
    "        destination_landmarks[ALIGN_POINTS])\n",
    "```\n",
    "\n",
    "2. Print the transformation matrix and analyse its different components.\n",
    "\n",
    "3. From this matrix, estimate the translation, the scaling factor and the angle of the rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = transformation_from_points(\n",
    "        source_landmarks[ALIGN_POINTS],\n",
    "        destination_landmarks[ALIGN_POINTS])\n",
    "\n",
    "print('Transformation matrix between the two faces:')\n",
    "print(M)\n",
    "print('')\n",
    "\n",
    "print('Translation:', M[0, 2], M[1, 2])\n",
    "print('')\n",
    "s = np.sqrt(M[0, 0]*M[1,1] - M[0, 1]*M[1, 0])\n",
    "print('Scaling:', s)\n",
    "print('')\n",
    "theta = np.arccos(M[0, 0]/s)\n",
    "print('Angle:', theta*180/np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 7:**  Extracting masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERLAY_POINTS = [\n",
    "    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
    "    NOSE_POINTS + MOUTH_POINTS\n",
    "]\n",
    "\n",
    "def get_face_mask(img, landmarks):\n",
    "    \"Extracts a mask on an image around the important regions.\"\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.float64)\n",
    "    # Compute the mask by computing the convex hull.\n",
    "    for group in OVERLAY_POINTS:\n",
    "        points = cv2.convexHull(landmarks[group])\n",
    "        cv2.fillConvexPoly(mask, points, color=1)\n",
    "    # Transform the mask into an image\n",
    "    mask = np.array([mask, mask, mask]).transpose((1, 2, 0))\n",
    "    # Blur the mask\n",
    "    mask = (cv2.GaussianBlur(mask, (11, 11), 0) > 0) * 1.0\n",
    "    mask = cv2.GaussianBlur(mask, (11, 11), 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the provided method to extract masks on the two images.\n",
    "\n",
    "2. Visualize the masks and the pixels under them by multiplying the image with the mask element per element.\n",
    "\n",
    "```python\n",
    "source_masked = source * source_mask\n",
    "```\n",
    "\n",
    "- Warning: you have to cast to the cropped image back to `np.uint8` to visualize it correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask over the source image\n",
    "source_mask = get_face_mask(source, source_landmarks)\n",
    "\n",
    "# Mask over the destination image\n",
    "destination_mask = get_face_mask(destination, destination_landmarks)\n",
    "\n",
    "# Apply the mask on the source image\n",
    "source_masked = source * source_mask\n",
    "\n",
    "# Apply the mask on the destination image\n",
    "destination_masked = destination * destination_mask\n",
    "\n",
    "# Plot the masks\n",
    "ax = plt.subplot(221)\n",
    "plt.imshow(source_mask)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "ax = plt.subplot(222)\n",
    "plt.imshow(destination_mask)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "ax = plt.subplot(223)\n",
    "plt.imshow(source_masked.astype(np.uint8))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "ax = plt.subplot(224)\n",
    "plt.imshow(destination_masked.astype(np.uint8))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 8:** Warping the source image\n",
    "\n",
    "- One can warp the source image to the destination using the similarity transform computed before: \n",
    "\n",
    "```python\n",
    "source_warped = cv2.warpAffine(source,\n",
    "                   M,\n",
    "                   (destination.shape[1], destination.shape[0]),\n",
    "                   dst=None,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT\n",
    "                   flags=cv2.WARP_INVERSE_MAP\n",
    "                )\n",
    "```\n",
    "\n",
    "1. Warp the source image to the destination and show both warped source and destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Warp the source image to the destination\n",
    "#######################################\n",
    "\n",
    "source_warped = cv2.warpAffine(source,\n",
    "                   M,\n",
    "                   (destination.shape[1], destination.shape[0]),\n",
    "                   dst=None,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
    "                   flags=cv2.WARP_INVERSE_MAP\n",
    "                )\n",
    "# Plot the warped images\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(source_warped)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(destination)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 9:** Blending the warped source image to the destination using the mask\n",
    "\n",
    "* One could simply \"cut\" the warped source image with the destination mask and compose the two images.\n",
    "\n",
    "* It is much more elegant to first warp the source mask and then \"add\" it to the destination mask.\n",
    "\n",
    "* The reason is that the Procrustes analysis is only an approximation, the warped source mask does not match exactly the destination mask.\n",
    "\n",
    "```python\n",
    "warped_mask = cv2.warpAffine(source_mask,\n",
    "                   M,\n",
    "                   (destination.shape[1], destination.shape[0]),\n",
    "                   dst=None,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
    "                   flags=cv2.WARP_INVERSE_MAP\n",
    "                )\n",
    "\n",
    "combined_mask = np.max([destination_mask, warped_mask],\n",
    "                          axis=0) \n",
    "```\n",
    "\n",
    "1. Warp the source mask to the destination\n",
    "\n",
    "2. Create the combined mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Warp the source mask to the destination\n",
    "#######################################\n",
    "\n",
    "# Warp the mask of the source image to the destination\n",
    "warped_mask = cv2.warpAffine(source_mask,\n",
    "                   M,\n",
    "                   (destination.shape[1], destination.shape[0]),\n",
    "                   dst=None,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
    "                   flags=cv2.WARP_INVERSE_MAP\n",
    "                )\n",
    "\n",
    "# Combine the warped source mask and the destination mask\n",
    "# This avoids \"missing\" pixels after composition\n",
    "combined_mask = np.max([destination_mask, warped_mask],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Crop the warped source using the combined mask.\n",
    "\n",
    "4. Crop the destination image using `(1 - combined_mask)`.\n",
    "\n",
    "5. Add the two images pixel by pixel (**blending**).\n",
    "\n",
    "6. Show the cropped source, the cropped destination and the composition.\n",
    "\n",
    "* The face regions are aligned, but the textures do not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Compose the warped source image with the destination using the mask\n",
    "#######################################\n",
    "\n",
    "# Crop the warped source image\n",
    "source_cropped = source_warped * combined_mask\n",
    "\n",
    "# Anti-crop the destination image\n",
    "destination_cropped = destination * (1 - combined_mask)\n",
    "\n",
    "# Add the two to compose the two images\n",
    "composition =  source_cropped + destination_cropped\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(source_cropped.astype(np.uint8))\n",
    "plt.title('Source')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(132)\n",
    "plt.imshow(destination_cropped.astype(np.uint8))\n",
    "plt.title('Destination')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(133)\n",
    "plt.imshow(composition.astype(np.uint8))\n",
    "plt.title('Composition')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 10:** Color correction\n",
    "\n",
    "* The last step is to correct the color balance of the warped source image to match the color profile of the destination image. \n",
    "\n",
    "* Matthew Earl proposes to use a variant of **RGB color balancing**:\n",
    "\n",
    "    1. The two images are filtered by a HUGE Gaussian filter (its width is equal to the distance between the eyes!). This forms a kind of local histogram of colors.\n",
    "\n",
    "    2. The color of the pixels of the warped source image are \"normalized\" to locally have the same distribution as the destination image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_colours(source, destination, landmarks):\n",
    "    \"RGB color scaling correction\"\n",
    "\n",
    "    # Compute the size of the Gaussian filter by measuring the distance between the eyes\n",
    "    blur_amount = 0.6*np.linalg.norm(\n",
    "                      np.mean(landmarks[LEFT_EYE_POINTS], axis=0) -\n",
    "                      np.mean(landmarks[RIGHT_EYE_POINTS], axis=0))\n",
    "    blur_amount = int(blur_amount)\n",
    "    if blur_amount % 2 == 0:\n",
    "        blur_amount += 1\n",
    "\n",
    "    # Blur the two images\n",
    "    destination_blur = cv2.GaussianBlur(destination, (blur_amount, blur_amount), 0)\n",
    "    source_blur = cv2.GaussianBlur(source, (blur_amount, blur_amount), 0)\n",
    "\n",
    "    # Avoid divide-by-zero errors.\n",
    "    source_blur += (128 * (source_blur <= 1.0)).astype(source_blur.dtype)\n",
    "\n",
    "    # Compute the color-corrected image\n",
    "    return (source.astype(np.float64) * destination_blur.astype(np.float64) \n",
    "                    / source_blur.astype(np.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the function `correct_colours`, correct the colors of the warped source image.\n",
    "\n",
    "2. Show the corrected source and the destination.\n",
    "\n",
    "3. Blend corrected source with the destination image.\n",
    "\n",
    "3. Renormalize the resulting image:\n",
    "\n",
    "```python\n",
    "composition = cv2.normalize(composition, None, 0.0, 255.0, cv2.NORM_MINMAX)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Correct the colors\n",
    "#######################################\n",
    "\n",
    "# Correct colors in the warped image\n",
    "source_warped_corrected = correct_colours(source_warped, destination, destination_landmarks)\n",
    "\n",
    "# Plot the color-matched images\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(source_warped_corrected.astype(np.uint8))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(destination)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Add the color-corrected warped image to the destination\n",
    "composition_corrected = destination * (1 - combined_mask) + source_warped_corrected * combined_mask\n",
    "\n",
    "# Normalize the image\n",
    "composition_corrected = cv2.normalize(composition_corrected, None, 0.0, 255.0, cv2.NORM_MINMAX)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(composition_corrected.astype(np.uint8))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 11:** Try it on other images\n",
    "\n",
    "- You are done! Now try it on other images.\n",
    "\n",
    "<img src=\"img/merkel_clinton.jpg\" alt=\"img/merkel_clinton.jpg\" width=\"300\"/> \n",
    "\n",
    "<img src=\"img/vitayhamker.png\" alt=\"img/vitayhamker.png\" width=\"600\"/> \n",
    "<img src=\"img/vamker.png\" alt=\"img/vamker.png\" width=\"300\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright notice:\n",
    "\n",
    "```python\n",
    "# Copyright (c) 2015 Matthew Earl\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#     The above copyright notice and this permission notice shall be included\n",
    "#     in all copies or substantial portions of the Software.\n",
    "#\n",
    "#     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n",
    "#     OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "#     MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n",
    "#     NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n",
    "#     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
    "#     OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n",
    "#     USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "\"\"\"\n",
    "This is the code behind the Switching Eds blog post:\n",
    "    \n",
    "    http://matthewearl.github.io/2015/07/28/switching-eds-with-python/\n",
    "\n",
    "The code is available at:\n",
    "\n",
    "    https://github.com/matthewearl/faceswap/blob/master/faceswap.py\n",
    "\n",
    "To run the script you'll need to install dlib (http://dlib.net) including its\n",
    "Python bindings, and OpenCV. \n",
    "\n",
    "You'll also need to obtain and unzip the trained model from sourceforge:\n",
    "\n",
    "    http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
